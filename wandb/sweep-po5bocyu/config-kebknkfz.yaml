wandb_version: 1

components.entity_ruler.ent_id_sep:
  value: '||'
components.entity_ruler.factory:
  value: entity_ruler
components.entity_ruler.overwrite_ents:
  value: 'false'
components.entity_ruler.validate:
  value: 'true'
components.ner.factory:
  value: ner
components.ner.model.@architectures:
  value: spacy.TransitionBasedParser.v2
components.ner.model.extra_state_tokens:
  value: 'false'
components.ner.model.hidden_width:
  value: 80
components.ner.model.maxout_pieces:
  value: 2
components.ner.model.state_type:
  value: ner
components.ner.model.tok2vec.@architectures:
  value: spacy.Tok2Vec.v2
components.ner.model.tok2vec.embed.@architectures:
  value: spacy.MultiHashEmbed.v2
components.ner.model.tok2vec.embed.include_static_vectors:
  value: 'false'
components.ner.model.tok2vec.embed.width:
  value: 166
components.ner.model.tok2vec.encode.@architectures:
  value: spacy.MaxoutWindowEncoder.v2
components.ner.model.tok2vec.encode.depth:
  value: 6
components.ner.model.tok2vec.encode.maxout_pieces:
  value: 2
components.ner.model.tok2vec.encode.width:
  value: 52
components.ner.model.tok2vec.encode.window_size:
  value: 1
components.ner.model.tok2vec.upstream:
  value: '*'
components.ner.model.tok2vec.width:
  value: 192
components.ner.model.use_upper:
  value: 'false'
components.ner.update_with_oracle_cut_size:
  value: 106
components.tok2vec.factory:
  value: tok2vec
components.tok2vec.model.@architectures:
  value: spacy.Tok2Vec.v1
components.tok2vec.model.embed.@architectures:
  value: spacy.MultiHashEmbed.v1
components.tok2vec.model.embed.include_static_vectors:
  value: 'false'
components.tok2vec.model.embed.width:
  value: 159
components.tok2vec.model.encode.@architectures:
  value: spacy.MaxoutWindowEncoder.v1
components.tok2vec.model.encode.depth:
  value: 4
components.tok2vec.model.encode.maxout_pieces:
  value: 5
components.tok2vec.model.encode.width:
  value: 159
components.tok2vec.model.encode.window_size:
  value: 2
corpora.dev.@readers:
  value: spacy.Corpus.v1
corpora.dev.gold_preproc:
  value: 'true'
corpora.dev.path:
  value: corpus/dev.spacy
corpora.train.@readers:
  value: spacy.Corpus.v1
corpora.train.gold_preproc:
  value: 'false'
corpora.train.max_length:
  value: 3055
corpora.train.path:
  value: corpus/train.spacy
initialize.components.entity_ruler.patterns.@readers:
  value: srsly.read_jsonl.v1
initialize.components.entity_ruler.patterns.path:
  value: training/custom_model_md/entity_ruler/patterns.jsonl
initialize.components.entity_ruler.patterns.skip:
  value: 'true'
initialize.lookups.@misc:
  value: spacy.LookupsDataLoader.v1
initialize.lookups.lang:
  value: en
initialize.vectors:
  value: training/custom_model_md
nlp.batch_size:
  value: 1004
nlp.lang:
  value: en
nlp.tokenizer.@tokenizers:
  value: spacy.Tokenizer.v1
paths.dev:
  value: corpus/dev.spacy
paths.train:
  value: corpus/train.spacy
paths.vectors:
  value: training/custom_model_md
training.accumulate_gradient:
  value: 1
training.batcher.@batchers:
  value: spacy.batch_by_words.v1
training.batcher.discard_oversize:
  value: 'true'
training.batcher.size.@schedules:
  value: compounding.v1
training.batcher.size.compound:
  value: 1.2085893840843576
training.batcher.size.start:
  value: 60
training.batcher.size.stop:
  value: 1685
training.batcher.tolerance:
  value: 0.18458248816482875
training.dev_corpus:
  value: corpora.dev
training.dropout:
  value: 0.1490387362556091
training.eval_frequency:
  value: 185
training.logger.@loggers:
  value: spacy.WandbLogger.v2
training.logger.log_dataset_dir:
  value: ./corpus
training.logger.model_log_interval:
  value: 1015
training.logger.project_name:
  value: NER-prodigy-logWandB
training.max_steps:
  value: 25091
training.optimizer.@optimizers:
  value: Adam.v1
training.optimizer.beta1:
  value: 0.5529299884777417
training.optimizer.beta2:
  value: 0.8345530163397397
training.optimizer.eps:
  value: 1.6067885117803736e-08
training.optimizer.grad_clip:
  value: 1
training.optimizer.l2:
  value: 0.010013701647246432
training.optimizer.l2_is_weight_decay:
  value: 'true'
training.optimizer.learn_rate.@schedules:
  value: warmup_linear.v1
training.optimizer.learn_rate.initial_rate:
  value: 9.435391380157194e-05
training.optimizer.learn_rate.total_steps:
  value: 17004
training.optimizer.learn_rate.warmup_steps:
  value: 332
training.optimizer.use_averages:
  value: 'false'
training.patience:
  value: 3153
training.score_weights.ents_f:
  value: 2
training.train_corpus:
  value: corpora.train
variables.wandb_project_name:
  value: NER-prodigy-logWandB
